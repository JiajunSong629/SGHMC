{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes our work on the optimization of SGHMC algorithm. We try several ways such as JIT with `Numba`, AOT with `Cython` and `C++` on simulation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import cython\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc( gradU, x0, C, V, n_samples=1000, n_steps=1, epsilon=0.1):\n",
    "    \n",
    "    np.random.seed(663)\n",
    "    \n",
    "    B = 0.5 * epsilon * V\n",
    "    C, V = np.asarray(C), np.asarray(V)\n",
    "    n_params = x0.shape[0]\n",
    "    D = np.sqrt(2*(C-B)*epsilon) if n_params == 1 else la.cholesky(2*(C-B)*epsilon) \n",
    "    \n",
    "    samples = np.zeros((n_samples, n_params))\n",
    "    samples[0] = x0\n",
    "    \n",
    "    for i in range(n_samples-1):\n",
    "        x = samples[i]\n",
    "        p = np.random.randn(n_params)\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            x = x + epsilon * p\n",
    "            p = p - epsilon * gradU(x) - epsilon * np.dot(C,p) - np.dot(D,random.randn(n_params))\n",
    "        \n",
    "        samples[i+1] = x\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = lambda x: -2 * x**2 + x**4\n",
    "gradU =  lambda x: - 4 * x +  4 * x**3 + np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = %prun -r -q sghmc(gradU, np.array([0]), C=3, V=4, n_samples=1000, n_steps=10)\n",
    "profile.sort_stats('cumtime').print_stats(10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `np.dot`, `np.random.randn`, lambda function of the gradient are called the most times during the execution. The profiling helps us to focus on the part of the function that needs improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we start to optimize the function, we store the true value and make sure the optimizaiton is correct firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = sghmc(gradU, np.array([0]), C=3, V=4, n_samples=1000, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sghmc(gradU, np.array([0]), C=3, V=4, n_samples=1000, n_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT with `numba`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `numba.jit` decorator which will trigger generation and execution of compiled code when the function is first called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def sghmc_numba( U, gradU, x0, C, V, n_samples=1000, n_steps=1, epsilon=0.1):\n",
    "    '''SGHMC Numba'''\n",
    "    \n",
    "    np.random.seed(663)\n",
    "    \n",
    "    B = 0.5 * epsilon * V\n",
    "    C, V = np.asarray(C), np.asarray(V)\n",
    "    n_params = x0.shape[0]\n",
    "    D = np.sqrt(2*(C-B)*epsilon) if n_params == 1 else la.cholesky(2*(C-B)*epsilon) \n",
    "    \n",
    "    samples = np.zeros((n_samples, n_params))\n",
    "    samples[0] = x0\n",
    "    \n",
    "    for i in range(n_samples-1):\n",
    "        x = samples[i]\n",
    "        p = np.random.randn(n_params)\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            x = x + epsilon * p\n",
    "            p = p - epsilon * gradU(x) - epsilon * np.dot(C,p) - np.dot(D,random.randn(n_params))\n",
    "        \n",
    "        samples[i+1] = x\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = sghmc_numba\n",
    "print(np.allclose(func(U, gradU, np.array([0]), C=3, V=4, n_samples=1000, n_steps=10), sol))\n",
    "\n",
    "%timeit func(U, gradU, np.array([0]), C=3, V=4, n_samples=1000, n_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to have significant improvement with Numba. Most likely because the function we write is not completely in plain Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOT with `Cython`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%cython` not found.\n"
     ]
    }
   ],
   "source": [
    "%%cython -a\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy.linalg as la\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def sghmc_cython_1(gradU, double[:] x0, double [:,:] C, double [:,:] V, int n_samples=1000, int n_steps=50, double epsilon=0.1):\n",
    "    '''SGHMC Cython'''\n",
    "    \n",
    "    cdef int n_params = x0.shape[0]\n",
    "    cdef double [:,:] B = np.empty((n_params, n_params))\n",
    "    cdef double [:,:] D = np.empty((n_params, n_params))\n",
    "    cdef double [:,:] samples = np.empty((n_samples, n_params))\n",
    "    cdef double [:] p = np.empty(n_params)\n",
    "    cdef double [:] x = np.empty(n_params)\n",
    "    cdef double [:] tmpgrad = np.empty(n_params)\n",
    "    cdef double [:] tmprand = np.empty(n_params)\n",
    "    cdef double [:] tmpdot = np.empty(n_params)\n",
    "    cdef int i, j, k, s, t\n",
    "    \n",
    "    np.random.seed(663)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            D[i,j] = 2*epsilon*C[i,j] - epsilon*epsilon*V[i,j]\n",
    "    \n",
    "    if n_params == 1:\n",
    "        D[0,0] = sqrt(D[0,0])\n",
    "    else:\n",
    "        D = la.cholesky(D)\n",
    "\n",
    "    samples[0, :] = x0    \n",
    "    for i in range(n_samples-1):\n",
    "        x = samples[i, :].copy()\n",
    "        p = np.random.randn(n_params)\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            for k in range(n_params):\n",
    "                x[k] = x[k] + epsilon*p[k]\n",
    "            \n",
    "            #p = p - epsilon * gradU_cython(x) - epsilon * np.dot(C,p) - np.dot(D,np.random.randn(n_params))\n",
    "            #tmpgrad = gradU_cython(x)\n",
    "            tmpgrad = gradU(np.asarray(x))\n",
    "            tmprand = np.random.randn(n_params)\n",
    "            tmpdot = np.zeros(n_params)\n",
    "            \n",
    "            for s in range(n_params):\n",
    "                for t in range(n_params):\n",
    "                    tmpdot[s] = tmpdot[s] + epsilon*C[s,t]*p[t] + D[s,t]*tmprand[t]\n",
    "            \n",
    "            for s in range(n_params):\n",
    "                p[s] = p[s] - tmpdot[s] - epsilon * tmpgrad[s]\n",
    "            \n",
    "            #for s in range(n_params):\n",
    "            #    tmpdot[s] = 0\n",
    "            \n",
    "        samples[i+1, :] = x\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like numpy under Cython will have different random mechanism. Thus we check the results by observing the posterior distribution of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sghmc_cython_1(gradU, x0 = np.array([0.0]), C=np.array([[3.0]]), V = np.array([[4.0]]), n_samples=10000, n_steps=50)\n",
    "plt.hist(np.asarray(x), bins=20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = %prun -r -q sghmc_cython_1(gradU, np.array([0.0]), C=np.array([[3.0]]), V=np.array([[4.0]]), \\\n",
    "                                     n_samples=1000, n_steps=10)\n",
    "profile.sort_stats('cumtime').print_stats()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sghmc_cython_1(gradU, np.array([0.0]), C=np.array([[3.0]]), V=np.array([[4.0]]), n_samples=1000, n_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is gradient the bottleneck?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the profling results we find out that the lambda function, namely the gradient calculation, is most time consuming. Thus we also implement it with Cython to see if we could boost the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy.linalg as la\n",
    "from libc.math cimport sqrt, pow\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cpdef gradU_cython(double [:] x):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double [:] y = np.empty(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        y[i] = -4*x[i] + 4*pow(x[i], 3) + np.random.randn()\n",
    "        \n",
    "    return y\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def sghmc_cython_2(double[:] x0, double [:,:] C, double [:,:] V, int n_samples=1000, int n_steps=50, double epsilon=0.1):\n",
    "    '''SGHMC Cython'''\n",
    "    \n",
    "    cdef int n_params = x0.shape[0]\n",
    "    cdef double [:,:] B = np.empty((n_params, n_params))\n",
    "    cdef double [:,:] D = np.empty((n_params, n_params))\n",
    "    cdef double [:,:] samples = np.empty((n_samples, n_params))\n",
    "    cdef double [:] p = np.empty(n_params)\n",
    "    cdef double [:] x = np.empty(n_params)\n",
    "    cdef double [:] tmpgrad = np.empty(n_params)\n",
    "    cdef double [:] tmprand = np.empty(n_params)\n",
    "    cdef double [:] tmpdot = np.empty(n_params)\n",
    "    cdef int i, j, k, s, t\n",
    "    \n",
    "    np.random.seed(663)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            D[i,j] = 2*epsilon*C[i,j] - epsilon*epsilon*V[i,j]\n",
    "    \n",
    "    if n_params == 1:\n",
    "        D[0,0] = sqrt(D[0,0])\n",
    "    else:\n",
    "        D = la.cholesky(D)\n",
    "\n",
    "    samples[0, :] = x0    \n",
    "    for i in range(n_samples-1):\n",
    "        x = samples[i, :].copy()\n",
    "        p = np.random.randn(n_params)\n",
    "        \n",
    "        for j in range(n_steps):\n",
    "            for k in range(n_params):\n",
    "                x[k] = x[k] + epsilon*p[k]\n",
    "            \n",
    "            tmpgrad = gradU_cython(x)\n",
    "            tmprand = np.random.randn(n_params)\n",
    "            tmpdot = np.zeros(n_params)\n",
    "            \n",
    "            for s in range(n_params):\n",
    "                for t in range(n_params):\n",
    "                    tmpdot[s] = tmpdot[s] + epsilon*C[s,t]*p[t] + D[s,t]*tmprand[t]\n",
    "            \n",
    "            for s in range(n_params):\n",
    "                p[s] = p[s] - tmpdot[s] - epsilon * tmpgrad[s]\n",
    "            \n",
    "        samples[i+1, :] = x\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sghmc_cython_2(np.array([0.0]), C=np.array([[3.0]]), V=np.array([[4.0]]), n_samples=1000, n_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the speed here is 2 times faster than in numpy, jit or Cython version without implementing the gradient. Note that in practice, we may not be able to implement the gradient or even don't have the analytical expression. The experiment here shows that bottleneck in speed results from the gradient calculation. One promising approach would be fully embed the algorithm into Tensorflow Probability because gradient calculation will be then based on tensor graph and run directly on optimized C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-writing critical functions in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file wrap.cpp\n",
    "<%\n",
    "cfg['compiler_args'] = ['-std=c++11']\n",
    "cfg['include_dirs'] = ['../eigen']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <stdexcept>\n",
    "#include <algorithm> // std::random_shuffle\n",
    "#include <random>\n",
    "\n",
    "#include <Eigen/LU>\n",
    "#include <Eigen/Dense>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "        \n",
    "// start random number engine with fixed seed\n",
    "default_random_engine re{1234};\n",
    "// set up random normal rnorm to work like in python\n",
    "normal_distribution<double> norm(0, 1); // mean and standard deviation\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "// fill xs with draws from N(0,1) and return this n x 1 dim vector\n",
    "Eigen::MatrixXd rnorm_vec(int n) {\n",
    "    Eigen::MatrixXd xs = Eigen::MatrixXd::Zero(n, 1);\n",
    "    for (int i=0; i<n; i++) {xs(i,0) = rnorm();}\n",
    "    return xs;\n",
    "}\n",
    "    \n",
    "// get noisy gradient of simulation 1\n",
    "Eigen::MatrixXd gradU_noisy(Eigen::MatrixXd x) {\n",
    "    Eigen::MatrixXd xs = -4*x.array() + 4*x.array().pow(3) + 2*rnorm_vec(x.rows()).array();\n",
    "    return xs;\n",
    "} \n",
    "\n",
    "// sghmc sampler\n",
    "Eigen::MatrixXd run_sghmc(Eigen::MatrixXd x0, Eigen::MatrixXd C, Eigen::MatrixXd V, int n_samples, int n_steps, double epsilon){\n",
    "    int n_params = x0.rows();\n",
    "    Eigen::MatrixXd gradU_batch = Eigen::MatrixXd::Zero(n_params, 1);\n",
    "    Eigen::MatrixXd samples = Eigen::MatrixXd::Zero(n_params, n_samples);\n",
    "    Eigen::MatrixXd B = 0.5 * epsilon * V;\n",
    "    Eigen::MatrixXd Sigma = 2.0 * epsilon * (C-B);\n",
    "    \n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfSig(Sigma);\n",
    "    Eigen::MatrixXd D = lltOfSig.matrixL();\n",
    "\n",
    "    Eigen::MatrixXd p = rnorm_vec(n_params);\n",
    "    Eigen::MatrixXd x = x0;\n",
    "    \n",
    "    samples.col(0) = x0;\n",
    "    for (int i=0; i<n_samples-1; i++) {\n",
    "        p = rnorm_vec(n_params);\n",
    "        \n",
    "        for (int j=0; j<n_steps; j++){\n",
    "            x = x + epsilon*p;\n",
    "            p = p - epsilon * gradU_noisy(x) - epsilon * (C*p) - D * rnorm_vec(n_params);\n",
    "        }\n",
    "        samples.col(i) = x;\n",
    "    }\n",
    "\n",
    "    return samples;\n",
    "}\n",
    "    \n",
    "PYBIND11_MODULE(wrap, m) {\n",
    "    m.doc() = \"auto-compiled c++ extension\";\n",
    "    m.def(\"run_sghmc\", &run_sghmc);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cppimport\n",
    "\n",
    "sghmc = cppimport.imp(\"wrap\")\n",
    "C = 3.0*np.eye(1)\n",
    "V = 4.0*np.eye(1)\n",
    "x0 = np.array([0.0])\n",
    "\n",
    "%timeit sghmc.run_sghmc(x0, C, V, 1000, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sghmc.run_sghmc(x0, C, V, 10000, 50, 0.1)\n",
    "samples_ = np.array(samples).reshape(-1, 1)\n",
    "plt.hist(samples_, bins=20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('.sghmc': venv)",
   "language": "python",
   "name": "python37564bitsghmcvenvadb4b442cff84c93b4eb62fd5a097173"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
